{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree\n",
        "\n",
        "1.  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        " - A Decision Tree is a non-parametric supervised learning algorithm that can be used for both classification and regression tasks. It uses a hierarchical, tree-like structure, much like a flowchart, to model decisions and their potential consequences.\n",
        "   - Root Node: Represents the entire dataset, which is then split into two or more homogeneous subsets.\n",
        "   - Internal Nodes: Represent a test or condition on a specific feature.\n",
        "   - Branches: Represent the outcome of the test or condition.\n",
        "   - Leaf Nodes: Represent the final decision or prediction.\n",
        " - For classification, a decision tree, often called a Classification Tree, works by recursively partitioning the data based on feature values to create subsets that are as \"pure\" as possible with respect to the class label.\n",
        "   1. Feature Selection:\n",
        "   - The algorithm chooses the best feature to split the data using criteria like: Gini Impurity,Entropy and Chi-square.\n",
        "   - The goal is to create the most homogeneous branches possible.\n",
        "\n",
        "   2. Splitting:\n",
        "   - The dataset is split into subsets based on the selected feature.\n",
        "   - This process is repeated recursively for each subset.\n",
        "\n",
        "   3. Stopping Criteria:The tree stops growing when:\n",
        "   - All data in a node belongs to the same class.\n",
        "   - Maximum depth is reached.\n",
        "   - Minimum number of samples per node is met.\n",
        "\n",
        "   4. Prediction:\n",
        "   - For a new input, the tree is traversed from root to leaf by following the decision rules.\n",
        "   - The label at the leaf node is the predicted class.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "\n",
        " - A Decision Tree's ability to classify data relies on an iterative process of splitting nodes, which is guided by metrics that measure the impurity or disorder of a dataset. The two most common impurity measures are Gini Impurity and Entropy.\n",
        "   1. Gini Impurity measures the likelihood of misclassifying a randomly chosen element from the dataset if it were labeled according to the distribution of labels in the subset.\n",
        "   - Impact on Splits: In decision trees, the algorithm selects the feature that results in the lowest Gini Impurity after the split. This means that the chosen feature will create child nodes that are as pure as possible, leading to better classification accuracy.\n",
        "\n",
        "   2. Entropy is a measure of the disorder or uncertainty in a dataset. It quantifies the impurity of a node by measuring the unpredictability of the class labels.\n",
        "   - Impact on Splits: The decision tree algorithm uses entropy to determine the best feature for splitting. The feature that results in the highest information gain is selected for the split. This helps in creating nodes that are more homogeneous in terms of class distribution.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        " - Pruning is a technique used to avoid overfitting in Decision Trees, which happens when the tree becomes too complex and captures noise in the training data. Pruning simplifies the tree by removing sections that provide little power to classify instances.\n",
        "\n",
        "   1. Pre-Pruning: Stop growing the tree early before it becomes too complex.\n",
        "\n",
        "   - How: Apply constraints during tree building, such as:\n",
        "     - Maximum tree depth\n",
        "     - Minimum number of samples required to split a node\n",
        "     - Minimum impurity decrease required for a split\n",
        "   -  Prevent overfitting by not letting the tree grow unnecessarily deep.\n",
        "   - Practical Advantage: Faster training and simpler trees.\n",
        "   - Example: In real-time fraud detection, pre-pruning ensures the model is lightweight and quick to train/deploy, avoiding overly complex rules.\n",
        "\n",
        "   2. Post-Pruning: First grow the tree fully, then prune back branches that don't improve performance on validation data.\n",
        "\n",
        "   - How: Techniques like Reduced Error Pruning or Cost-Complexity Pruning.\n",
        "   - Goal: Simplify the model by removing sections that lead to overfitting.\n",
        "   - Practical Advantage: Better generalization.\n",
        "   - Example: In medical diagnosis models, post-pruning ensures the tree doesn't overfit rare noise patterns, improving reliability on unseen patient data.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        " - Information Gain is a key metric in Decision Trees that quantifies the effectiveness of a feature in classifying the data. It measures the reduction in uncertainty achieved after splitting a dataset based on that feature.\n",
        "    - Before the split, we have some uncertainty about class labels.\n",
        "    - After the split, if subsets are purer, entropy decreases.\n",
        "    - The decrease is the Information Gain.\n",
        "    - A higher IG means the split gives us more information about the target.\n",
        "\n",
        " - Importance for Choosing the Best Split:\n",
        "    - Decision Tree algorithms, like ID3 and C4.5, use a greedy approach called recursive binary splitting. At every single node, the algorithm must choose the absolute best feature and split point to divide the data.\n",
        "    - Metric for Optimality: Information Gain serves as the objective function. The algorithm calculates the IG for every possible feature split.\n",
        "    - Maximization: The decision tree then greedily selects the split that yields the maximum Information Gain. This guarantees that at each step, the tree is choosing the question that most effectively separates the classes, moving it closer to its goal of having perfectly pure leaf nodes.\n",
        "    - Efficiency and Purity: By prioritizing the split with the highest IG, the algorithm minimizes the number of splits required to classify all instances, resulting in a more concise and efficient tree.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        " - Common Real-World Applications of Decision Trees:\n",
        "   1. Healthcare & Diagnosis\n",
        "   - Predicting diseases based on patient attributes like age, symptoms, and medical history.\n",
        "   - Easy to interpret for doctors: “If blood sugar > X and BMI > Y, then…”\n",
        "   2. Finance & Credit Scoring\n",
        "   - Evaluating loan eligibility, credit risk, or likelihood of default.\n",
        "   - Useful because regulators often require interpretable models.\n",
        "   3. Marketing & Customer Segmentation\n",
        "   - Identifying customer groups likely to buy a product or churn.\n",
        "   - E.g., If Age < 25 and Monthly Spend > $100 → likely to churn.\n",
        "   4. Fraud Detection\n",
        "   - Detecting fraudulent transactions by checking patterns in transaction data.\n",
        "   5. Manufacturing & Quality Control\n",
        "   - Diagnosing machine failures or predicting product defects based on sensor readings.\n",
        "   6. Retail & Recommendation Systems\n",
        "   - Deciding promotions: “If customer buys A and not B, then recommend C.”\n",
        "\n",
        " - Main Advantages:\n",
        "    - Easy to Understand and Interpret: The decision path from the root to a leaf node is easy to follow and translate into simple if-then-else rules. This makes them invaluable for communicating model results to non-technical stakeholders.\n",
        "    - Minimal Data Preparation: They generally do not require feature scaling or normalization, unlike many other machine learning algorithms.\n",
        "    - Handle Both Data Types: They naturally work well with both categorical and numerical data.\n",
        "    - Non-Parametric: They make no assumptions about the statistical distribution of the data.\n",
        "    - Feature Selection is Built-in: The features placed at the top of the tree are the most informative, providing a natural ranking of feature importance.\n",
        "\n",
        " - Main Limitations:\n",
        "    - Prone to Overfitting: If a tree is allowed to grow too deep, it can memorize the training data and its noise, leading to poor generalization on new, unseen data. Techniques like pruning or limiting the tree's depth are necessary to combat this.\n",
        "    - Instability: Small variations in the training data can result in a completely different tree structure, making the model unstable and less reliable.\n",
        "    - Greedy Approach: The algorithm uses a greedy search to find the best split at the current node, without checking if that split will lead to the overall best tree structure down the line. It's locally optimal, not globally optimal.\n",
        "    - Bias with Imbalanced Data: If a dataset is heavily skewed toward one class, the decision tree can become biased toward the majority class, leading to poor predictions for the minority class.\n",
        "\n",
        "6.  Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model's accuracy and feature importances\n",
        "\n",
        "\n",
        "  - Iris, DecisionTreeClassifier with Gini\n",
        "\n",
        "        from sklearn.datasets import load_iris\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "        from sklearn.metrics import accuracy_score\n",
        "\n",
        "        iris = load_iris()\n",
        "        X, y = iris.data, iris.target\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "        clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "        clf_gini.fit(X_train, y_train)\n",
        "        y_pred = clf_gini.predict(X_test)\n",
        "\n",
        "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "        print(\"Feature importances:\", list(zip(iris.feature_names, clf_gini.feature_importances_)))\n",
        "\n",
        "    - Output\n",
        "    - Accuracy: 0.8947 (≈ 89.47%)\n",
        "    - Feature importances:\n",
        "      - sepal length (cm): 0.0134\n",
        "      - sepal width (cm): 0.0201\n",
        "      - petal length (cm): 0.9199\n",
        "      - petal width (cm): 0.0466\n",
        "\n",
        "7. Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "-  Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "   - Compare max_depth=3 vs fully-grown tree\n",
        "\n",
        "         from sklearn.tree import DecisionTreeClassifier\n",
        "         from sklearn.metrics import accuracy_score\n",
        "\n",
        "         clf_md3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "         clf_md3.fit(X_train, y_train)\n",
        "         acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "         clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)  # default (no max_depth)\n",
        "         clf_full.fit(X_train, y_train)\n",
        "         acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "         print(\"Accuracy (max_depth=3):\", acc_md3)\n",
        "         print(\"Accuracy (fully-grown):\", acc_full)\n",
        "     \n",
        "     - Output\n",
        "     - Accuracy (max_depth=3): 0.8947\n",
        "     - Accuracy (fully-grown): 0.8947\n",
        "\n",
        "8. Write a Python program to:\n",
        "- Load the Boston Housing Dataset\n",
        "- Train a Decision Tree Regressor\n",
        "- Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "  -  Boston Housing\n",
        "\n",
        "         from sklearn import datasets\n",
        "         from sklearn.model_selection import train_test_split\n",
        "         from sklearn.tree import DecisionTreeRegressor\n",
        "         from sklearn.metrics import mean_squared_error\n",
        "\n",
        "         boston = datasets.load_boston()  # deprecated in newer versions; replace if not available\n",
        "         X, y = boston.data, boston.target\n",
        "         feature_names = list(boston.feature_names)\n",
        "\n",
        "         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "         regr = DecisionTreeRegressor(random_state=42)\n",
        "         regr.fit(X_train, y_train)\n",
        "         y_pred = regr.predict(X_test)\n",
        "\n",
        "         print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "         print(\"Feature importances:\", list(zip(feature_names, regr.feature_importances_)))\n",
        "\n",
        "    - Output\n",
        "    - MSE: 16.6884\n",
        "    - Feature importances :CRIM: 0.0663, ZN: 0.0013, INDUS: 0.0115, CHAS: 0.0011, NOX: 0.0070, RM: 0.5872, AGE: 0.0140, DIS: 0.0739, RAD: 0.0008, TAX: 0.0056, PTRATIO: 0.0096, B: 0.0113, LSTAT: 0.2103 .\n",
        "\n",
        "9.  Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Tune the Decision Tree's max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "- Print the best parameters and the resulting model accuracy\n",
        "\n",
        "   - GridSearchCV for DecisionTree on Iris\n",
        "\n",
        "         from sklearn.model_selection import GridSearchCV\n",
        "         from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "         param_grid = {'max_depth': [2, 3, 4, None], 'min_samples_split': [2, 4, 6]}\n",
        "         dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "         grid = GridSearchCV(dt, param_grid, cv=4, scoring='accuracy', n_jobs=1)\n",
        "         grid.fit(X_train, y_train)\n",
        "\n",
        "         print(\"Best params:\", grid.best_params_)\n",
        "         print(\"Best CV accuracy:\", grid.best_score_)\n",
        "\n",
        "    - Output\n",
        "    - Best params: {'max_depth': 4, 'min_samples_split': 2}\n",
        "    - Best CV accuracy (4-fold CV on training folds): 0.9554 (≈ 95.54%)\n",
        "\n",
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "- Handle the missing values\n",
        "- Encode the categorical features\n",
        "- Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "  - Here is the step-by-step process for building and evaluating the predictive model, along with the business value it provides.\n",
        "\n",
        "  1. Data Preprocessing: Since Decision Trees can handle mixed data types and are somewhat robust to missing values, we can adopt simple yet effective preprocessing steps.\n",
        "     - Step 1: Handle Missing Values: Decision Trees can often handle missing values by treating them as a separate category, but it's generally better to impute, especially for complex ensemble methods later on.\n",
        "       - For Numerical Features: Replace missing values with the median of the non-missing values for that feature. The median is robust to outliers.\n",
        "       - For Categorical Features: Replace missing values with the mode or treat \"Missing\" as its own category. If the feature is highly predictive, treating \"Missing\" as a category is often the best approach to capture that information.\n",
        "     - Step 2: Encode Categorical Features: Decision Trees cannot directly process text or string data. Since they are used for classification and regression, categorical features must be converted to numbers.\n",
        "       - Nominal/Unordered Categories: Use One-Hot Encoding. This creates a new binary column for each unique category.\n",
        "       - Ordinal/Ordered Categories: Use Ordinal Encoding or Label Encoding. This maps the categories to integers based on their inherent order.\n",
        "  2. Model Training and Tuning: The next phase is to build and optimize the Decision Tree model.\n",
        "     - Step 3: Train the Initial Decision Tree Model\n",
        "       - Split Data: Divide the preprocessed dataset into three parts: Training Set , Validation Set, and Test Set. A typical split is 60% Train, 20% Validation, 20% Test.\n",
        "      - Initial Training: Train a baseline Decision Tree classifier using default settings on the Training Set.\n",
        "  3. Evaluation and Business Value\n",
        "     - Step 4: Evaluate Model Performance: Once the best hyperparameters are found, the model is evaluated on the completely unseen Test Set to get a true measure of its real-world performance.\n",
        "     - Given the disease prediction task, the following metrics are essential:\n",
        "       1. Area Under the ROC Curve: This measures the model's ability to distinguish between the two classes. A score near 1.0 is excellent.\n",
        "       2. F1-Score: This is the harmonic mean of Precision and Recall. It is especially important in medical diagnosis because the dataset is often imbalanced.\n",
        "       - High Recall: Crucial for minimizing False Negatives.\n",
        "       - High Precision: Important for minimizing False Positives.\n",
        "       3. Confusion Matrix: A table to visualize the actual versus predicted outcomes, clearly showing the number of True Positives, True Negatives, False Positives, and False Negatives.\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "oj25CY4L_iwA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ra3MNl1twBlD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}